---
title: "Joshua Burrows Project 2"
date: "16 October 2020"
output: 
  github_document: 
    toc: TRUE  
    toc_depth: 4  
params: 
  day: "Monday"
---

```{r Setup, include = FALSE}
library(knitr)
library(rmarkdown)
library(tidyverse)
library(caret)
library(corrplot)
library(shiny)
library(styler)
opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, cache = TRUE, tidy = "styler")
```

# Bike Rentals on `r paste0(params$day, "s")`: Introduction

This document walks though the process of creating a model to predict the number of bikes that will be rented on `r paste0(tolower(params$day), "s")`.

I compared two models - a *non-ensemble tree* and a *boosted tree* - and picked the one that does better. These models use the following predictor variables: 

- yr: year (2011 or 2012)  
- mnth: month  
- hr: hour of the day  
- holiday: whether the day is a holiday  
- weathersit: weather condition  
    + pleasant: clear, few clouds, partly cloudy  
    + less pleasant: mist, mist + cloudy, mist + broken clouds, mist + few clouds  
    + even less pleasant: light snow, light Rain + scattered clouds, light rain + thunderstorm + scattered clouds  
    + downright unpleasant: snow + fog, heavy rain + ice pallets + thunderstorm + mist  
- temp: normalized temperature in celsius  
- hum: normalized humidity  
- windspeed: normalized windspeed  

You can return to the homepage for this project by clicking [here](README.md). 

# Read in Data 

## Get Bikes Data 

Read in data that has been downloaded from [the UCI Machine Learning Library](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). 

```{r Read}
bikes <- read_csv(file = "../Bike-Sharing-Dataset/hour.csv")

bikes %>% head() %>% kable()
```

## Factors 

Convert categorical variables to factors. 

```{r Factorize} 
bikes$weekday <- as.factor(bikes$weekday)
levels(bikes$weekday) <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday") 

bikes$season <- as.factor(bikes$season)
levels(bikes$season) <- c("winter", "spring", "summer", "fall")

bikes$yr <- as.factor(bikes$yr)
levels(bikes$yr) <- c("2011", "2012")

bikes$mnth <- as.factor(bikes$mnth)
levels(bikes$mnth) <- c("jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")

bikes$weathersit <- as.factor(bikes$weathersit)
levels(bikes$weathersit) <- c("pleasant", "less pleasant", "even less pleasant", "downright unpleasant")

bikes$holiday <- as.factor(bikes$holiday)
levels(bikes$holiday) <- c("no", "yes")

bikes$workingday <- as.factor(bikes$workingday)
levels(bikes$workingday) <- c("no", "yes")

bikes %>% head() %>% kable()
```

## Split by Day 

Grab the data for `r tolower(params$day)`. 

```{r Split}
dayData <- bikes %>% filter(weekday == params$day)

dayData %>% head() %>% kable()
```

# Exploratory Data Analysis  

I started with a little bit of exploratory data analysis. The goal is to look at the relationships between the predictors and number of bike rentals. 

## Quantitative Summaries and Plots 

### Correlations 

Create a correlation plot for the quantitative predictors. 

*atemp* represents the heat index, which is typically calculated using temperature and humidity. So it makes sense to either eliminate *atemp* from the model or keep *atemp* but eliminate *temp* and *hum*. I decided to eliminate *atemp*. 

```{r Correlation}
corr <- dayData %>% select(temp, atemp, windspeed, hum) %>% cor()

corrplot(corr)
```

### Hour 

Create a scatter plot to investigate the relationship between time of day and rentals on `r paste0(tolower(params$day), "s")`. Fit a line through the points to get a basic idea of their relationship. 

```{r Hour}
avgRentals <- dayData %>% group_by(hr) %>% summarize(meanRentals = mean(cnt))

ggplot(avgRentals, aes(x = hr, y = meanRentals)) + geom_point() + labs(title = paste0("Total Rentals on ", paste0(params$day, "s"), " by Hour"), x = "Hour of the Day", y = "Total Rentals") + geom_smooth()
```

### Temperature

Create a scatter plot to investigate the relationship between temperature and number of rentals on `r paste0(tolower(params$day), "s")`. Fit a line through the points to get a basic idea of their relationship. 

The size of the dots represents the number of observations at each temperature. 

```{r Temp}
tempAvg <- dayData %>% group_by(temp) %>% summarize(avgRentals = mean(cnt), n = n())

ggplot(tempAvg, aes(x = temp, y = avgRentals)) + geom_point(aes(size = n)) + geom_smooth() + labs(title = paste0("Average Rentals on ", paste0(params$day, "s"), " by Temperature"), x = "Normalized Temperature", y = "Average Rentals") + scale_size_continuous(name = "Number of Obs")
```

### Felt Temperature 

Create a scatter plot to investigate the relationship between felt temperature and number of rentals on `r paste0(tolower(params$day), "s")`. Fit a line through the points to get a basic idea of their relationship. 

The size of the dots represents the number of observations at each felt temperatrure. 

As already noted, it does not make much sense to keep *atemp* if *temp* and *hum* will be in the model, so I eliminated *atemp* from the model. 

```{r aTemp}
atempAvg <- dayData %>% group_by(atemp) %>% summarize(avgRentals = mean(cnt), n = n())

ggplot(atempAvg, aes(x = atemp, y = avgRentals)) + geom_point(aes(size = n)) + geom_smooth() + labs(title = paste0("Average Rentals on ", paste0(params$day, "s"), " by Felt Temperature"), x = "Normalized Feeling Temperature", y = "Average Rentals") + scale_size_continuous(name = "Number of Obs")
```

### Humidity

Create a scatter plot to investigate the relationship between humidity and number of rentals on `r paste0(tolower(params$day), "s")`. Fit a line through the points to get a basic idea of their relationship. 

The size of the dots represents the number of observations at each humidity level. 

Note: There are no holidays on Saturday or Sunday because the holiday data has been extracted from the [Washington D.C. HR Department's Holiday Schedule](https://dchr.dc.gov/page/holiday-schedules), which only lists holidays that fall during the work week. 

```{r Hum}
humAvg <- dayData %>% group_by(hum) %>% summarize(avgRentals = mean(cnt), n = n())

ggplot(humAvg, aes(x = hum, y = avgRentals)) + geom_point(aes(size = n)) + geom_smooth() + labs(title = paste0("Average Rentals on ", paste0(params$day, "s"), " by Humidity"), x = "Normalized Humidity", y = "Average Rentals") + scale_size_continuous(name = "Number of Obs")
```

### Windspeed 

Create a scatter plot to investigate the relationship between windspeed and number of rentals on `r paste0(tolower(params$day), "s")`. Fit a line through the points to get a basic idea of their relationship. 

The size of the dots represents the number of observations at each windspeed. 

```{r Wind}
windAvg <- dayData %>% group_by(windspeed) %>% summarize(avgRentals = mean(cnt), n = n())

ggplot(windAvg, aes(x = windspeed, y = avgRentals)) + geom_point(aes(size = n)) + geom_smooth() + labs(title = paste0("Average Rentals on ", paste0(params$day, "s"), " by Windspeed"), x = "Normalized Windspeed", y = "Average Rentals") + scale_size_continuous(name = "Number of Obs")
```

## Categorical Summaries and Plots

Explore the relationship between the predictors and number of bikes rented on `paste0(tolower(params$day), "s")` by creating some basic summaries and plots. 

### Helper Function 

Create a helper function to display basic numeric summaries for a given grouping variable. 

```{r Helper Function} 
getSum <- function(varName, colName){ 
  
  sum <- dayData %>% group_by(dayData[[varName]]) %>% summarize(min = min(cnt), Q1 = quantile(cnt, probs = c(.25), names = FALSE), median = median(cnt), mean = mean(cnt), Q3 = quantile(cnt, probs = c(.75), names = FALSE), max = max(cnt), obs = n())
  
  output <- sum %>% kable(col.names = c(colName, "Minimum", "1st Quartile", "Median", "Mean", "3rd Quartile", "Maximum", "Number of Observations"))
  
  return(output)
  
} 
```

### Season 

Explore how bike rentals on `r paste0(tolower(params$day), "s")` change with the seasons using a basic numeric summary and a boxplot. The boxplot can be used to identify outliers. 

It does not make much sense to keep both *season* and *mnth* in the model, so I decided to eliminate *season*. 

```{r Season}
getSum(varName = "season", colName = "Season")

ggplot(dayData, aes(x = season, y = cnt)) + geom_boxplot() + labs(title = paste0("Rentals on ", paste0(params$day, "s"), " by Season"), x = "Season", y = "Number of Rentals") 
```

### Year 

Looking at total rentals each year gives us some idea of the long term trend in bike rentals on `r paste0(tolower(params$day), "s")`. It would be helpful to have data from more years. 

```{r Year}
yearSum <- dayData %>% group_by(yr) %>% summarize(totalRentals = sum(cnt))

yearSum %>% kable(col.names = c("Year", "Total Rentals"))
```

### Month

Explore how bike rentals on `r paste0(tolower(params$day), "s")` change depending on the month.  

As already noted, it is probably not worth including *mnth* and *season* in the model, so *season* has been eliminated. 

```{r Month}
getSum(varName = "mnth", colName = "Month")

ggplot(dayData, aes(x = mnth, y = cnt)) + geom_boxplot() + labs(title = paste0("Rentals on ", paste0(params$day, "s"), " by Month"), x = "Month", y = "Number of Rentals")
```


### Holiday 

Explore change in bike rentals depending on whether the `r tolower(params$day)` in question is a holiday. 

```{r Holiday}
getSum(varName = "holiday", colName = "Holiday")

ggplot(dayData, aes(x = holiday, y = cnt)) + geom_boxplot() + labs(title = paste0("Rentals on ", paste0(params$day, "s"), " by Holiday"), x = "Is it a Holiday?", y = "Number of Rentals")
```

### Working Day 

Average rentals by working day. 

Working days are neither weekends nor holidays. I decided not to keep this variable in the model because it wouldn't make much sense in the reports for Saturday and Sunday. 

```{r Workingday}
getSum(varName = "workingday", colName = "Working Day")

ggplot(dayData, aes(x = workingday, y = cnt)) + geom_boxplot() + labs(title = paste0("Rentals on ", paste0(params$day, "s"), " by Working Day"), x = "Is it a Working Day?", y = "Number of Rentals")
```

### Weather Condition 

Explore how bike rentals on `r paste0(tolower(params$day), "s")` change depending on the weather. 

```{r Weather}
getSum(varName = "weathersit", colName = "Weather Condition")

ggplot(dayData, aes(x = weathersit, y = cnt)) + geom_boxplot() + labs(title = paste0("Rentals on ", paste0(params$day, "s"), " by Weather Condition"), x = "What is the Weather Like?", y = "Number of Rentals")
```

# Train Models

After exploring the data, I created two models, a non-ensemble tree and a boosted tree. 

## Split Data

Split the data into a training set and a test set. The training set is used to build the models, and the test set is used to evaluate them. 

```{r Split into Train and Test}
set.seed(123)
trainIndex <- createDataPartition(dayData$cnt, p = .75, list = FALSE)

train <- dayData[trainIndex,]
test <- dayData[-trainIndex,]
```

## Non-Ensemble Tree

### Training

Fit a non-ensemble tree model. 

#### Tree Models 

Tree models split each predictor space into regions and make a different prediction for each region. For example, suppose we are interested in predicting life expectancy based on exercise habits. We might split the predictor space into **exercises less than one hour a week** and **exercises at least one hour a week** and then predict that people in the second group live longer. 

How do we decide whether to split at one hour, one and a half hours, two hours, etc? This decision is made using a method called "Recursive Binary Splitting", which we don't have to worry about too much because the *caret* package does it for us. 

Ensemble tree models fit lots of trees and then average their results. Here I have created a basic non-ensemble tree to model bicycle rentals. 

#### Tuning Parameter

This model has one "tuning parameter" called *cp*. *cp* stands for "Complexity Parameter", and it controls the number of "nodes" that the tree has. 

The life expectancy example above has two terminal nodes: **less than one hour** and **at least one hour**. We could complicate the example by adding additional nodes. For instance, we could divide the group **less than one hour** into two subgroups: **less than a half hour** and **greater than half an hour but less than one hour**. And we could divide **at least one hour a week** into **less than two hours** and **greater than two hours**. 

Sometimes increasing the number of nodes makes your model better, but sometimes it makes it worse. There are lots of different methods for picking the best number of nodes. For the bicycle rental model, I used a method called "Leave One Out Cross Validation". 

*LOOCV* works by removing an observation from the data set, using the rest of the data to create a model, and then seeing how well that model does at predicting the observation that was left out. This process is repeated for every observation, and the results are combined. 

If we want to compare two different values of *cp*, we will go through the *LOOCV* process twice and compare the results. In this way, we can test different values of *cp* to see which one performs best. 

I used the *caret* package to test 10 different values of *cp*. 

#### Create the Model 

```{r Train Tree}
set.seed(123)
tree <- train(cnt ~ yr + mnth + hr + holiday + weathersit + temp + hum + windspeed, 
              data = train, 
              method = "rpart", 
              trControl = trainControl(method = "LOOCV"), 
              tuneLength = 10)
```

### Model Information 

My final non-ensemble tree model uses a *cp* of `r tree$bestTune$cp`. Its root mean square error on the training set is `r min(tree$results$RMSE)`. 

More information about this model is below.  

```{r Train Tree Info}
tree
plot(tree$finalModel)
text(tree$finalModel)
```


## Boosted Tree 

### Training 

#### Boosted Tree Models 

Boosted trees are another type of tree model. "Boosting" works by fitting a series of trees, each of which is a modified version of the previous tree. The idea is to hone in on the best model. 

#### Tuning Paremeters 

Four tuning parameters are involved:  
- *n.trees*: number of boosting iterations  
- *interaction.depth*: maximum tree depth  
- *shrinkage*: how strongly each subsequent tree is influenced by the previous tree  
- *n.minobsinnode*: minimum terminal node size  

Values for the tuning parameters are found using Cross Validation. Cross Validation works by splitting the data into groups called "folds". One fold is left out, the rest are used to create a model, and then that model is tested on the fold that was left out. This process is repeated for each fold, and the results are combined. It should be clear that *LOOCV* is just *CV* with the number of folds equal to the number of observations. 

I used the *caret* package to test 81 different combinations of tuning parameters. 

#### Create the Model 

```{r Boosted Tree}
tuneGr <- expand.grid(n.trees = seq(from = 50, to = 150, by = 50), 
                     interaction.depth = 1:3, 
                     shrinkage = seq(from = .05, to = .15, by = .05), 
                     n.minobsinnode = 9:11)

set.seed(123)
boostTree <- train(cnt ~ yr + mnth + hr + holiday + weathersit + temp + hum + windspeed, 
                   data = train, 
                   method = "gbm", 
                   trControl = trainControl(method = "cv", number = 10),
                   tuneGrid = tuneGr, 
                   verbose = FALSE)
```

### Model Information 

My final boosted tree model uses the following tuning parameters:  

- *n.trees*: `r boostTree$bestTune$n.trees`  
- *interaction.depth*: `r boostTree$bestTune$interaction.depth`  
- *shrinkage*: `r boostTree$bestTune$shrinkage`  
- *n.minobsinnode*: `r boostTree$bestTune$n.minobsinnode`  

Its root mean square error on the training set is `r min(boostTree$results$RMSE)`. 

More information about this model is below.  

```{r Train boostTree Info}
boostTree
```

# Test Models  

Test the models on the test set. Select the model that performs better. 

Performance is measured using Root Mean Square Error, which is a measure of how close the model gets to correctly predicting the test data. The RMSE for each model is displayed below. 

```{r Test Tree}
treePreds <- predict(tree, test)
treeRMSE <- postResample(treePreds, test$cnt)[1]

boostPreds <- predict(boostTree, test)
boostRMSE <- postResample(boostPreds, test$cnt)[1]

modelPerformance <- data.frame(model = c("Non-Ensemble Tree", "Boosted Tree"), trainRMSE = c(min(tree$results$RMSE), min(boostTree$results$RMSE)), testRMSE = c(treeRMSE, boostRMSE))

modelPerformance %>% kable(col.names = c("Model", "Train RMSE", "Test RMSE"))
```

# Best Model

```{r Best Model}
best <- modelPerformance %>% filter(testRMSE == min(testRMSE))
worst <- modelPerformance %>% filter(testRMSE == max(testRMSE))
```

The `r tolower(best$model)` performs better than the `r tolower(worst$model)` as judged by RMSE on the test set. 

The `r tolower(best$model)` model is saved to the `final` object below. 

```{r Final}
if(best$model == "Non-Ensemble Tree"){
  final <- tree
} else if(best$model == "Boosted Tree"){
  final <- boostTree
} else{
  stop("Error")
}

final$finalModel
```





